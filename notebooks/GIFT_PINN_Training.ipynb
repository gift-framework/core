{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIFT-Native PINN Training Notebook\n",
    "\n",
    "Train a Physics-Informed Neural Network with built-in GIFT algebraic structure.\n",
    "\n",
    "**Runnable on Google Colab with free T4/A100 GPU.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "from fractions import Fraction\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GIFT Constants (Hard-coded, proven in Lean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GIFT CONSTANTS\n",
    "# =============================================================================\n",
    "\n",
    "B2 = 21           # Second Betti number = C(7,2)\n",
    "B3 = 77           # Third Betti number\n",
    "DIM_G2 = 14       # dim(G2)\n",
    "H_STAR = B2 + B3 + 1  # = 99\n",
    "\n",
    "DET_G_TARGET = Fraction(65, 32)  # = 2.03125\n",
    "DET_G_TARGET_FLOAT = float(DET_G_TARGET)\n",
    "\n",
    "TORSION_THRESHOLD = 0.0288  # Joyce threshold\n",
    "PINN_TARGET_TORSION = 0.001  # Our target (20x margin)\n",
    "\n",
    "# Fano plane lines (cyclic triples)\n",
    "FANO_LINES = [\n",
    "    (0, 1, 3), (1, 2, 4), (2, 3, 5), (3, 4, 6),\n",
    "    (4, 5, 0), (5, 6, 1), (6, 0, 2),\n",
    "]\n",
    "\n",
    "print(\"GIFT Constants (Proven in Lean)\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"b2 (Second Betti number): {B2}\")\n",
    "print(f\"b3 (Third Betti number): {B3}\")\n",
    "print(f\"H* = b2 + b3 + 1 = {H_STAR}\")\n",
    "print(f\"dim(G2) = {DIM_G2}\")\n",
    "print(f\"det(g) target = 65/32 = {DET_G_TARGET_FLOAT:.6f}\")\n",
    "print(f\"Joyce torsion threshold = {TORSION_THRESHOLD}\")\n",
    "print()\n",
    "print(\"Fano plane lines:\")\n",
    "for i, line in enumerate(FANO_LINES):\n",
    "    print(f\"  Line {i}: {line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Epsilon Tensor from Fano Plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_epsilon_tensor():\n",
    "    \"\"\"Build structure constants epsilon_ijk from Fano plane.\"\"\"\n",
    "    epsilon = np.zeros((7, 7, 7), dtype=np.float32)\n",
    "    for (i, j, k) in FANO_LINES:\n",
    "        # Cyclic: +1\n",
    "        epsilon[i, j, k] = epsilon[j, k, i] = epsilon[k, i, j] = 1\n",
    "        # Anti-cyclic: -1\n",
    "        epsilon[j, i, k] = epsilon[i, k, j] = epsilon[k, j, i] = -1\n",
    "    return epsilon\n",
    "\n",
    "EPSILON = build_epsilon_tensor()\n",
    "print(f\"Epsilon tensor shape: {EPSILON.shape}\")\n",
    "print(f\"Non-zero entries: {np.count_nonzero(EPSILON)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi0_standard(normalize=True):\n",
    "    \"\"\"Standard G2 3-form (35 independent components).\"\"\"\n",
    "    phi0 = []\n",
    "    for i in range(7):\n",
    "        for j in range(i + 1, 7):\n",
    "            for k in range(j + 1, 7):\n",
    "                phi0.append(EPSILON[i, j, k])\n",
    "    phi0 = np.array(phi0, dtype=np.float32)\n",
    "    if normalize:\n",
    "        scale = (65.0 / 32.0) ** (1.0 / 7.0)\n",
    "        phi0 = phi0 * scale\n",
    "    return phi0\n",
    "\n",
    "phi0 = phi0_standard()\n",
    "print(f\"phi0 has {len(phi0)} components (C(7,3) = 35)\")\n",
    "print(f\"Non-zero: {np.sum(np.abs(phi0) > 1e-10)}\")\n",
    "print(f\"Norm: {np.linalg.norm(phi0):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierFeatures(nn.Module):\n",
    "    \"\"\"Random Fourier feature encoding.\"\"\"\n",
    "    def __init__(self, input_dim=7, num_frequencies=32, scale=1.0):\n",
    "        super().__init__()\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.output_dim = 2 * num_frequencies\n",
    "        B = torch.randn(num_frequencies, input_dim) * scale\n",
    "        self.register_buffer('B', B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected = 2 * np.pi * torch.matmul(x, self.B.T)\n",
    "        return torch.cat([torch.cos(projected), torch.sin(projected)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g2_generators():\n",
    "    \"\"\"Compute 14 generators of G2 in so(7).\"\"\"\n",
    "    generators = np.zeros((14, 7, 7), dtype=np.float32)\n",
    "    # First 7: rotations in Fano planes\n",
    "    for idx, (i, j, k) in enumerate(FANO_LINES):\n",
    "        generators[idx, i, j] = 1\n",
    "        generators[idx, j, i] = -1\n",
    "    # Remaining 7: mixed rotations\n",
    "    for idx in range(7):\n",
    "        i, j, k = idx, (idx + 1) % 7, (idx + 3) % 7\n",
    "        gen_idx = 7 + idx\n",
    "        generators[gen_idx, i, k] = 1\n",
    "        generators[gen_idx, k, i] = -1\n",
    "        generators[gen_idx, j, k] = 0.5\n",
    "        generators[gen_idx, k, j] = -0.5\n",
    "    # Normalize\n",
    "    for idx in range(14):\n",
    "        norm = np.linalg.norm(generators[idx])\n",
    "        if norm > 1e-10:\n",
    "            generators[idx] /= norm\n",
    "    return generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GIFT-Native PINN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIFTNativePINN(nn.Module):\n",
    "    \"\"\"\n",
    "    PINN with GIFT structure built-in.\n",
    "    - Hard-coded Fano epsilon_ijk\n",
    "    - G2 adjoint: 14 DOF (not 35)\n",
    "    - phi = phi0 + scale * delta_phi\n",
    "    \"\"\"\n",
    "    def __init__(self, num_frequencies=32, hidden_dims=None, perturbation_scale=0.01):\n",
    "        super().__init__()\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [128, 128, 128]\n",
    "        \n",
    "        self.perturbation_scale = perturbation_scale\n",
    "        \n",
    "        # Register buffers\n",
    "        self.register_buffer('epsilon', torch.from_numpy(EPSILON))\n",
    "        self.register_buffer('phi0', torch.from_numpy(phi0_standard()))\n",
    "        \n",
    "        # Precompute Lie derivatives\n",
    "        self._precompute_lie_derivatives()\n",
    "        \n",
    "        # Network\n",
    "        self.fourier = FourierFeatures(input_dim=7, num_frequencies=num_frequencies)\n",
    "        \n",
    "        layers = []\n",
    "        in_dim = self.fourier.output_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.extend([nn.Linear(in_dim, h_dim), nn.SiLU()])\n",
    "            in_dim = h_dim\n",
    "        layers.append(nn.Linear(in_dim, 14))  # 14 G2 adjoint params\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _precompute_lie_derivatives(self):\n",
    "        generators = g2_generators()\n",
    "        lie_derivs = np.zeros((14, 35), dtype=np.float32)\n",
    "        for a in range(14):\n",
    "            X = generators[a]\n",
    "            idx = 0\n",
    "            for i in range(7):\n",
    "                for j in range(i+1, 7):\n",
    "                    for k in range(j+1, 7):\n",
    "                        val = sum(X[i,l]*EPSILON[l,j,k] + X[j,l]*EPSILON[i,l,k] + X[k,l]*EPSILON[i,j,l] for l in range(7))\n",
    "                        lie_derivs[a, idx] = val\n",
    "                        idx += 1\n",
    "        self.register_buffer('lie_derivatives', torch.from_numpy(lie_derivs))\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=0.1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.fourier(x)\n",
    "        adjoint = self.mlp(h)  # (N, 14)\n",
    "        delta_phi = torch.matmul(adjoint, self.lie_derivatives)  # (N, 35)\n",
    "        return self.phi0.unsqueeze(0) + self.perturbation_scale * delta_phi\n",
    "    \n",
    "    def phi_tensor(self, x):\n",
    "        \"\"\"Get full (N, 7, 7, 7) tensor.\"\"\"\n",
    "        components = self.forward(x)\n",
    "        N = components.shape[0]\n",
    "        phi = torch.zeros(N, 7, 7, 7, device=x.device, dtype=x.dtype)\n",
    "        idx = 0\n",
    "        for i in range(7):\n",
    "            for j in range(i+1, 7):\n",
    "                for k in range(j+1, 7):\n",
    "                    val = components[:, idx]\n",
    "                    phi[:,i,j,k] = phi[:,j,k,i] = phi[:,k,i,j] = val\n",
    "                    phi[:,j,i,k] = phi[:,i,k,j] = phi[:,k,j,i] = -val\n",
    "                    idx += 1\n",
    "        return phi\n",
    "    \n",
    "    def metric(self, x):\n",
    "        phi = self.phi_tensor(x)\n",
    "        return torch.einsum('nikl,njlm->nij', phi, phi) / 36.0\n",
    "    \n",
    "    def det_g(self, x):\n",
    "        return torch.linalg.det(self.metric(x))\n",
    "    \n",
    "    def get_adjoint_params(self, x):\n",
    "        return self.mlp(self.fourier(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIFTLoss(nn.Module):\n",
    "    def __init__(self, det_weight=100.0, torsion_weight=1.0, sparse_weight=0.1, pd_weight=10.0):\n",
    "        super().__init__()\n",
    "        self.det_weight = det_weight\n",
    "        self.torsion_weight = torsion_weight\n",
    "        self.sparse_weight = sparse_weight\n",
    "        self.pd_weight = pd_weight\n",
    "    \n",
    "    def forward(self, model, x, return_components=False):\n",
    "        losses = {}\n",
    "        \n",
    "        # 1. Determinant loss\n",
    "        det_g = model.det_g(x)\n",
    "        losses['det'] = torch.mean((det_g - DET_G_TARGET_FLOAT) ** 2)\n",
    "        \n",
    "        # 2. Torsion loss (gradient-based proxy)\n",
    "        x_grad = x.clone().requires_grad_(True)\n",
    "        phi = model(x_grad)\n",
    "        torsion = 0.0\n",
    "        for i in range(min(10, 35)):  # Sample components for speed\n",
    "            grad = torch.autograd.grad(phi[:, i].sum(), x_grad, create_graph=True, retain_graph=True)[0]\n",
    "            torsion = torsion + (grad ** 2).sum(dim=-1).mean()\n",
    "        losses['torsion'] = torsion / 10.0\n",
    "        \n",
    "        # 3. Sparsity\n",
    "        adjoint = model.get_adjoint_params(x)\n",
    "        losses['sparse'] = torch.mean(adjoint ** 2)\n",
    "        \n",
    "        # 4. Positive definiteness\n",
    "        g = model.metric(x)\n",
    "        eigvals = torch.linalg.eigvalsh(g)\n",
    "        losses['pd'] = torch.mean(torch.relu(-eigvals) ** 2)\n",
    "        \n",
    "        total = (self.det_weight * losses['det'] + \n",
    "                 self.torsion_weight * losses['torsion'] +\n",
    "                 self.sparse_weight * losses['sparse'] +\n",
    "                 self.pd_weight * losses['pd'])\n",
    "        \n",
    "        return (total, losses) if return_components else total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GIFTNativePINN(\n",
    "    num_frequencies=32,\n",
    "    hidden_dims=[128, 128, 128],\n",
    "    perturbation_scale=0.01,\n",
    ").to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")\n",
    "print(f\"Architecture: 7D -> Fourier(64) -> MLP -> 14 (G2 adjoint) -> 35 (3-form)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "x_test = torch.rand(100, 7, device=device)\n",
    "with torch.no_grad():\n",
    "    phi_test = model(x_test)\n",
    "    det_test = model.det_g(x_test)\n",
    "\n",
    "print(f\"Input: {x_test.shape}\")\n",
    "print(f\"Output (phi): {phi_test.shape}\")\n",
    "print(f\"det(g) mean: {det_test.mean().item():.6f} (target: {DET_G_TARGET_FLOAT:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "EPOCHS = 3000\n",
    "BATCH_SIZE = 512\n",
    "LR = 1e-3\n",
    "TARGET_TORSION = 0.001\n",
    "TARGET_DET_ERROR = 1e-5\n",
    "\n",
    "loss_fn = GIFTLoss(det_weight=100.0, torsion_weight=1.0, sparse_weight=0.1, pd_weight=10.0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=200)\n",
    "\n",
    "history = {'loss': [], 'torsion': [], 'det_error': [], 'lr': []}\n",
    "best_torsion = float('inf')\n",
    "best_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "pbar = tqdm(range(EPOCHS), desc=\"Training\")\n",
    "\n",
    "for epoch in pbar:\n",
    "    x = torch.rand(BATCH_SIZE, 7, device=device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss, components = loss_fn(model, x, return_components=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "    \n",
    "    # Record\n",
    "    history['loss'].append(loss.item())\n",
    "    history['torsion'].append(components['torsion'].item())\n",
    "    history['det_error'].append(components['det'].item())\n",
    "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    if components['torsion'].item() < best_torsion:\n",
    "        best_torsion = components['torsion'].item()\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    pbar.set_postfix({\n",
    "        'loss': f\"{loss.item():.4f}\",\n",
    "        'torsion': f\"{components['torsion'].item():.6f}\",\n",
    "        'det_err': f\"{components['det'].item():.6f}\",\n",
    "    })\n",
    "    \n",
    "    if components['torsion'].item() < TARGET_TORSION and components['det'].item() < TARGET_DET_ERROR:\n",
    "        print(f\"\\nConverged at epoch {epoch}!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest torsion: {best_torsion:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "axes[0,0].semilogy(history['loss'])\n",
    "axes[0,0].set_xlabel('Epoch'); axes[0,0].set_ylabel('Loss'); axes[0,0].set_title('Total Loss')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0,1].semilogy(history['torsion'], label='Torsion')\n",
    "axes[0,1].axhline(TARGET_TORSION, color='g', linestyle='--', label=f'Target {TARGET_TORSION}')\n",
    "axes[0,1].axhline(TORSION_THRESHOLD, color='r', linestyle='--', label=f'Joyce {TORSION_THRESHOLD}')\n",
    "axes[0,1].set_xlabel('Epoch'); axes[0,1].set_ylabel('Torsion'); axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1,0].semilogy(history['det_error'])\n",
    "axes[1,0].set_xlabel('Epoch'); axes[1,0].set_ylabel('|det(g) - 65/32|Â²'); axes[1,0].set_title('Det Error')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1,1].semilogy(history['lr'])\n",
    "axes[1,1].set_xlabel('Epoch'); axes[1,1].set_ylabel('LR'); axes[1,1].set_title('Learning Rate')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "if best_state:\n",
    "    model.load_state_dict(best_state)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_eval = torch.rand(10000, 7, device=device)\n",
    "    det_eval = model.det_g(x_eval)\n",
    "    adjoint_eval = model.get_adjoint_params(x_eval)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"det(g) mean: {det_eval.mean().item():.8f}\")\n",
    "print(f\"det(g) target: {DET_G_TARGET_FLOAT:.8f}\")\n",
    "print(f\"det(g) error: {abs(det_eval.mean().item() - DET_G_TARGET_FLOAT):.2e}\")\n",
    "print(f\"Best torsion: {best_torsion:.6f}\")\n",
    "print(f\"Joyce threshold: {TORSION_THRESHOLD}\")\n",
    "print(f\"Margin: {TORSION_THRESHOLD / best_torsion:.1f}x\" if best_torsion > 0 else \"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(det_eval.cpu().numpy(), bins=50, density=True, alpha=0.7)\n",
    "axes[0].axvline(DET_G_TARGET_FLOAT, color='r', linestyle='--', label='Target 65/32')\n",
    "axes[0].set_xlabel('det(g)'); axes[0].set_title('det(g) Distribution'); axes[0].legend()\n",
    "\n",
    "axes[1].hist(adjoint_eval.cpu().numpy().flatten(), bins=50, density=True, alpha=0.7)\n",
    "axes[1].set_xlabel('Adjoint params'); axes[1].set_title('G2 Adjoint Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'best_state_dict': best_state,\n",
    "    'history': history,\n",
    "    'best_torsion': best_torsion,\n",
    "    'config': {\n",
    "        'num_frequencies': 32,\n",
    "        'hidden_dims': [128, 128, 128],\n",
    "        'perturbation_scale': 0.01,\n",
    "    }\n",
    "}, 'gift_pinn_trained.pt')\n",
    "\n",
    "print(\"Model saved to: gift_pinn_trained.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torsion_ok = best_torsion < TARGET_TORSION\n",
    "det_ok = abs(det_eval.mean().item() - DET_G_TARGET_FLOAT) < TARGET_DET_ERROR\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  GIFT-Native PINN Training Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  [{'X' if torsion_ok else ' '}] Torsion < {TARGET_TORSION}: {best_torsion:.6f}\")\n",
    "print(f\"  [{'X' if det_ok else ' '}] |det(g) - 65/32| < {TARGET_DET_ERROR}: {abs(det_eval.mean().item() - DET_G_TARGET_FLOAT):.2e}\")\n",
    "print()\n",
    "if torsion_ok and det_ok:\n",
    "    print(\"SUCCESS: All criteria met!\")\n",
    "else:\n",
    "    print(\"Training may need more epochs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
