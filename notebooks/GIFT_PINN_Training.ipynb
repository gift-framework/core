{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type># GIFT-Native PINN Training Notebook (v3.1.4 - CORRECTED)\n\nTrain a Physics-Informed Neural Network with built-in GIFT algebraic structure.\n\n**Runnable on Google Colab with free T4/A100 GPU.**\n\n## Key Fixes in This Version\n\n1. **Correct φ₀ definition** (from `G2Holonomy.lean`):\n   - φ₀ = e¹²³ + e¹⁴⁵ + e¹⁶⁷ + e²⁴⁶ - e²⁵⁷ - e³⁴⁷ - e³⁵⁶\n   - This is DIFFERENT from Fano plane cross-product structure!\n\n2. **Correct metric formula**:\n   - g_ij = (1/6) Σ_{k,l} φ_ikl · φ_jkl (contract SAME indices)\n   - NOT: g_ij = (1/36) Σ_{klm} φ_ikl · φ_jlm (wrong!)\n\n3. **Correct scaling**:\n   - c^14 = 65/32 → c = (65/32)^{1/14} ≈ 1.0543\n   - NOT: (65/32)^{1/7} (wrong!)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional\n",
    "from fractions import Fraction\n",
    "\n",
    "# Check GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GIFT Constants (Hard-coded, proven in Lean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GIFT CONSTANTS\n",
    "# =============================================================================\n",
    "\n",
    "B2 = 21           # Second Betti number = C(7,2)\n",
    "B3 = 77           # Third Betti number\n",
    "DIM_G2 = 14       # dim(G2)\n",
    "H_STAR = B2 + B3 + 1  # = 99\n",
    "\n",
    "DET_G_TARGET = Fraction(65, 32)  # = 2.03125\n",
    "DET_G_TARGET_FLOAT = float(DET_G_TARGET)\n",
    "\n",
    "TORSION_THRESHOLD = 0.0288  # Joyce threshold\n",
    "PINN_TARGET_TORSION = 0.001  # Our target (20x margin)\n",
    "\n",
    "# Fano plane lines (cyclic triples)\n",
    "FANO_LINES = [\n",
    "    (0, 1, 3), (1, 2, 4), (2, 3, 5), (3, 4, 6),\n",
    "    (4, 5, 0), (5, 6, 1), (6, 0, 2),\n",
    "]\n",
    "\n",
    "print(\"GIFT Constants (Proven in Lean)\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"b2 (Second Betti number): {B2}\")\n",
    "print(f\"b3 (Third Betti number): {B3}\")\n",
    "print(f\"H* = b2 + b3 + 1 = {H_STAR}\")\n",
    "print(f\"dim(G2) = {DIM_G2}\")\n",
    "print(f\"det(g) target = 65/32 = {DET_G_TARGET_FLOAT:.6f}\")\n",
    "print(f\"Joyce torsion threshold = {TORSION_THRESHOLD}\")\n",
    "print()\n",
    "print(\"Fano plane lines:\")\n",
    "for i, line in enumerate(FANO_LINES):\n",
    "    print(f\"  Line {i}: {line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 3. Standard G2 3-form (from Lean proof G2Holonomy.lean)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Standard G2 3-form (from Lean proof G2Holonomy.lean, lines 36-40)\n# φ₀ = e¹²³ + e¹⁴⁵ + e¹⁶⁷ + e²⁴⁶ - e²⁵⁷ - e³⁴⁷ - e³⁵⁶\n#\n# IMPORTANT: This is DIFFERENT from the Fano plane cross-product structure!\n# The Fano lines define octonion multiplication, but φ₀ is the associative 3-form.\n\nSTANDARD_G2_FORM = [\n    ((0, 1, 2), +1.0),  # e^123\n    ((0, 3, 4), +1.0),  # e^145\n    ((0, 5, 6), +1.0),  # e^167\n    ((1, 3, 5), +1.0),  # e^246\n    ((1, 4, 6), -1.0),  # e^257\n    ((2, 3, 6), -1.0),  # e^347\n    ((2, 4, 5), -1.0),  # e^356\n]\n\ndef _form_index_3(i, j, k):\n    \"\"\"Map (i, j, k) with i < j < k to linear index in C(7,3) = 35.\"\"\"\n    count = 0\n    for a in range(7):\n        for b in range(a + 1, 7):\n            for c in range(b + 1, 7):\n                if (a, b, c) == (i, j, k):\n                    return count\n                count += 1\n    raise ValueError(f\"Invalid indices: {i}, {j}, {k}\")\n\ndef build_phi0_tensor():\n    \"\"\"Build full 7×7×7 antisymmetric tensor for standard G2 form.\"\"\"\n    phi0 = np.zeros((7, 7, 7), dtype=np.float32)\n    for (indices, sign) in STANDARD_G2_FORM:\n        i, j, k = indices\n        phi0[i,j,k] = phi0[j,k,i] = phi0[k,i,j] = sign\n        phi0[j,i,k] = phi0[i,k,j] = phi0[k,j,i] = -sign\n    return phi0\n\nPHI0_TENSOR = build_phi0_tensor()\nprint(f\"φ₀ tensor shape: {PHI0_TENSOR.shape}\")\nprint(f\"Non-zero entries: {np.count_nonzero(PHI0_TENSOR)} (7 terms × 6 permutations = 42)\")\nprint()\nprint(\"Standard G2 form terms:\")\nfor (indices, sign) in STANDARD_G2_FORM:\n    sign_str = \"+\" if sign > 0 else \"-\"\n    print(f\"  {sign_str}e^{{{indices[0]+1}{indices[1]+1}{indices[2]+1}}}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def phi0_standard(normalize=True):\n    \"\"\"\n    Standard G2 3-form (35 independent components).\n    \n    For the STANDARD form, the induced metric is IDENTITY on ℝ⁷, so det(g) = 1.\n    \n    To achieve det(g) = 65/32:\n    - If φ → c·φ, then g → c²·g (since g_ij ~ φ_ikl φ_jkl)\n    - Therefore det(g) → c^14 · det(g)\n    - We need c^14 = 65/32, so c = (65/32)^{1/14} ≈ 1.0543\n    \"\"\"\n    phi0 = np.zeros(35, dtype=np.float32)\n    for (indices, sign) in STANDARD_G2_FORM:\n        idx = _form_index_3(*indices)\n        phi0[idx] = sign\n    \n    if normalize:\n        # c^14 = 65/32 → c = (65/32)^{1/14}\n        scale = (65.0 / 32.0) ** (1.0 / 14.0)\n        phi0 = phi0 * scale\n    \n    return phi0\n\nphi0 = phi0_standard()\nprint(f\"φ₀ has {len(phi0)} components (C(7,3) = 35)\")\nprint(f\"Non-zero: {np.sum(np.abs(phi0) > 1e-10)} (the 7 standard terms)\")\nprint(f\"Scale factor: {(65.0/32.0)**(1/14):.6f}\")\nprint()\nprint(\"Verification:\")\nprint(f\"  Expected det(g) = 65/32 = {65/32:.6f}\")\n# Quick sanity check with the formula g_ij = (1/6) sum_{k,l} phi_ikl * phi_jkl\ng_test = np.einsum('ikl,jkl->ij', PHI0_TENSOR, PHI0_TENSOR) / 6.0\nprint(f\"  Computed g (diagonal): {np.diag(g_test)}\")\nprint(f\"  det(g) for standard φ₀: {np.linalg.det(g_test):.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourierFeatures(nn.Module):\n",
    "    \"\"\"Random Fourier feature encoding.\"\"\"\n",
    "    def __init__(self, input_dim=7, num_frequencies=32, scale=1.0):\n",
    "        super().__init__()\n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.output_dim = 2 * num_frequencies\n",
    "        B = torch.randn(num_frequencies, input_dim) * scale\n",
    "        self.register_buffer('B', B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected = 2 * np.pi * torch.matmul(x, self.B.T)\n",
    "        return torch.cat([torch.cos(projected), torch.sin(projected)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def g2_generators():\n    \"\"\"Compute 14 generators of G2 in so(7).\"\"\"\n    generators = np.zeros((14, 7, 7), dtype=np.float32)\n    # First 7: rotations in Fano planes\n    for idx, (i, j, k) in enumerate(FANO_LINES):\n        generators[idx, i, j] = 1\n        generators[idx, j, i] = -1\n    # Remaining 7: mixed rotations\n    for idx in range(7):\n        i, j, k = idx, (idx + 1) % 7, (idx + 3) % 7\n        gen_idx = 7 + idx\n        generators[gen_idx, i, k] = 1\n        generators[gen_idx, k, i] = -1\n        generators[gen_idx, j, k] = 0.5\n        generators[gen_idx, k, j] = -0.5\n    # Normalize\n    for idx in range(14):\n        norm = np.linalg.norm(generators[idx])\n        if norm > 1e-10:\n            generators[idx] /= norm\n    return generators\n\ndef precompute_lie_derivatives():\n    \"\"\"\n    Compute Lie derivatives of φ₀ along G2 generators.\n    \n    CRITICAL: Use PHI0_TENSOR (correct G2 form), NOT Fano epsilon!\n    This was a bug in the previous version.\n    \"\"\"\n    generators = g2_generators()\n    lie_derivs = np.zeros((14, 35), dtype=np.float32)\n    for a in range(14):\n        X = generators[a]\n        idx = 0\n        for i in range(7):\n            for j in range(i+1, 7):\n                for k in range(j+1, 7):\n                    # (L_X φ)_{ijk} = X_i^l φ_{ljk} + X_j^l φ_{ilk} + X_k^l φ_{ijl}\n                    val = sum(\n                        X[i,l]*PHI0_TENSOR[l,j,k] + \n                        X[j,l]*PHI0_TENSOR[i,l,k] + \n                        X[k,l]*PHI0_TENSOR[i,j,l] \n                        for l in range(7)\n                    )\n                    lie_derivs[a, idx] = val\n                    idx += 1\n    return lie_derivs\n\nLIE_DERIVATIVES = precompute_lie_derivatives()\nprint(f\"Lie derivatives shape: {LIE_DERIVATIVES.shape}\")\nprint(f\"Non-zero Lie derivative entries: {np.count_nonzero(np.abs(LIE_DERIVATIVES) > 1e-10)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GIFT-Native PINN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class GIFTNativePINN(nn.Module):\n    \"\"\"\n    PINN with GIFT structure built-in.\n    \n    Key features:\n    - Uses correct G2 3-form φ₀ (not Fano lines!)\n    - G2 adjoint: 14 DOF (not 35)\n    - phi = phi0 + scale * delta_phi\n    - Correct metric formula: g_ij = (1/6) Σ_{k,l} φ_ikl φ_jkl\n    \"\"\"\n    def __init__(self, num_frequencies=32, hidden_dims=None, perturbation_scale=0.01):\n        super().__init__()\n        if hidden_dims is None:\n            hidden_dims = [128, 128, 128]\n        \n        self.perturbation_scale = perturbation_scale\n        \n        # Register buffers - use correct G2 form\n        self.register_buffer('phi0_tensor', torch.from_numpy(PHI0_TENSOR))\n        self.register_buffer('phi0', torch.from_numpy(phi0_standard()))\n        self.register_buffer('lie_derivatives', torch.from_numpy(LIE_DERIVATIVES))\n        \n        # Network\n        self.fourier = FourierFeatures(input_dim=7, num_frequencies=num_frequencies)\n        \n        layers = []\n        in_dim = self.fourier.output_dim\n        for h_dim in hidden_dims:\n            layers.extend([nn.Linear(in_dim, h_dim), nn.SiLU()])\n            in_dim = h_dim\n        layers.append(nn.Linear(in_dim, 14))  # 14 G2 adjoint params\n        self.mlp = nn.Sequential(*layers)\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight, gain=0.1)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n    \n    def forward(self, x):\n        h = self.fourier(x)\n        adjoint = self.mlp(h)  # (N, 14)\n        delta_phi = torch.matmul(adjoint, self.lie_derivatives)  # (N, 35)\n        return self.phi0.unsqueeze(0) + self.perturbation_scale * delta_phi\n    \n    def phi_tensor(self, x):\n        \"\"\"Get full (N, 7, 7, 7) antisymmetric tensor.\"\"\"\n        components = self.forward(x)\n        N = components.shape[0]\n        phi = torch.zeros(N, 7, 7, 7, device=x.device, dtype=x.dtype)\n        idx = 0\n        for i in range(7):\n            for j in range(i+1, 7):\n                for k in range(j+1, 7):\n                    val = components[:, idx]\n                    phi[:,i,j,k] = phi[:,j,k,i] = phi[:,k,i,j] = val\n                    phi[:,j,i,k] = phi[:,i,k,j] = phi[:,k,j,i] = -val\n                    idx += 1\n        return phi\n    \n    def metric(self, x):\n        \"\"\"\n        Compute metric g_ij from φ.\n        \n        CORRECT formula (from g2_form.py and Bryant 1987):\n            g_ij = (1/6) Σ_{k,l} φ_ikl · φ_jkl\n        \n        Note: Contract over SAME indices k,l (not different indices!)\n        For standard φ₀, this gives g = identity.\n        \"\"\"\n        phi = self.phi_tensor(x)\n        # Contract over same indices k and l\n        return torch.einsum('nikl,njkl->nij', phi, phi) / 6.0\n    \n    def det_g(self, x):\n        return torch.linalg.det(self.metric(x))\n    \n    def get_adjoint_params(self, x):\n        return self.mlp(self.fourier(x))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIFTLoss(nn.Module):\n",
    "    def __init__(self, det_weight=100.0, torsion_weight=1.0, sparse_weight=0.1, pd_weight=10.0):\n",
    "        super().__init__()\n",
    "        self.det_weight = det_weight\n",
    "        self.torsion_weight = torsion_weight\n",
    "        self.sparse_weight = sparse_weight\n",
    "        self.pd_weight = pd_weight\n",
    "    \n",
    "    def forward(self, model, x, return_components=False):\n",
    "        losses = {}\n",
    "        \n",
    "        # 1. Determinant loss\n",
    "        det_g = model.det_g(x)\n",
    "        losses['det'] = torch.mean((det_g - DET_G_TARGET_FLOAT) ** 2)\n",
    "        \n",
    "        # 2. Torsion loss (gradient-based proxy)\n",
    "        x_grad = x.clone().requires_grad_(True)\n",
    "        phi = model(x_grad)\n",
    "        torsion = 0.0\n",
    "        for i in range(min(10, 35)):  # Sample components for speed\n",
    "            grad = torch.autograd.grad(phi[:, i].sum(), x_grad, create_graph=True, retain_graph=True)[0]\n",
    "            torsion = torsion + (grad ** 2).sum(dim=-1).mean()\n",
    "        losses['torsion'] = torsion / 10.0\n",
    "        \n",
    "        # 3. Sparsity\n",
    "        adjoint = model.get_adjoint_params(x)\n",
    "        losses['sparse'] = torch.mean(adjoint ** 2)\n",
    "        \n",
    "        # 4. Positive definiteness\n",
    "        g = model.metric(x)\n",
    "        eigvals = torch.linalg.eigvalsh(g)\n",
    "        losses['pd'] = torch.mean(torch.relu(-eigvals) ** 2)\n",
    "        \n",
    "        total = (self.det_weight * losses['det'] + \n",
    "                 self.torsion_weight * losses['torsion'] +\n",
    "                 self.sparse_weight * losses['sparse'] +\n",
    "                 self.pd_weight * losses['pd'])\n",
    "        \n",
    "        return (total, losses) if return_components else total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GIFTNativePINN(\n",
    "    num_frequencies=32,\n",
    "    hidden_dims=[128, 128, 128],\n",
    "    perturbation_scale=0.01,\n",
    ").to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")\n",
    "print(f\"Architecture: 7D -> Fourier(64) -> MLP -> 14 (G2 adjoint) -> 35 (3-form)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "x_test = torch.rand(100, 7, device=device)\n",
    "with torch.no_grad():\n",
    "    phi_test = model(x_test)\n",
    "    det_test = model.det_g(x_test)\n",
    "\n",
    "print(f\"Input: {x_test.shape}\")\n",
    "print(f\"Output (phi): {phi_test.shape}\")\n",
    "print(f\"det(g) mean: {det_test.mean().item():.6f} (target: {DET_G_TARGET_FLOAT:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "EPOCHS = 3000\n",
    "BATCH_SIZE = 512\n",
    "LR = 1e-3\n",
    "TARGET_TORSION = 0.001\n",
    "TARGET_DET_ERROR = 1e-5\n",
    "\n",
    "loss_fn = GIFTLoss(det_weight=100.0, torsion_weight=1.0, sparse_weight=0.1, pd_weight=10.0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=200)\n",
    "\n",
    "history = {'loss': [], 'torsion': [], 'det_error': [], 'lr': []}\n",
    "best_torsion = float('inf')\n",
    "best_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "pbar = tqdm(range(EPOCHS), desc=\"Training\")\n",
    "\n",
    "for epoch in pbar:\n",
    "    x = torch.rand(BATCH_SIZE, 7, device=device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss, components = loss_fn(model, x, return_components=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "    \n",
    "    # Record\n",
    "    history['loss'].append(loss.item())\n",
    "    history['torsion'].append(components['torsion'].item())\n",
    "    history['det_error'].append(components['det'].item())\n",
    "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    if components['torsion'].item() < best_torsion:\n",
    "        best_torsion = components['torsion'].item()\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    pbar.set_postfix({\n",
    "        'loss': f\"{loss.item():.4f}\",\n",
    "        'torsion': f\"{components['torsion'].item():.6f}\",\n",
    "        'det_err': f\"{components['det'].item():.6f}\",\n",
    "    })\n",
    "    \n",
    "    if components['torsion'].item() < TARGET_TORSION and components['det'].item() < TARGET_DET_ERROR:\n",
    "        print(f\"\\nConverged at epoch {epoch}!\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest torsion: {best_torsion:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "axes[0,0].semilogy(history['loss'])\n",
    "axes[0,0].set_xlabel('Epoch'); axes[0,0].set_ylabel('Loss'); axes[0,0].set_title('Total Loss')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0,1].semilogy(history['torsion'], label='Torsion')\n",
    "axes[0,1].axhline(TARGET_TORSION, color='g', linestyle='--', label=f'Target {TARGET_TORSION}')\n",
    "axes[0,1].axhline(TORSION_THRESHOLD, color='r', linestyle='--', label=f'Joyce {TORSION_THRESHOLD}')\n",
    "axes[0,1].set_xlabel('Epoch'); axes[0,1].set_ylabel('Torsion'); axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1,0].semilogy(history['det_error'])\n",
    "axes[1,0].set_xlabel('Epoch'); axes[1,0].set_ylabel('|det(g) - 65/32|²'); axes[1,0].set_title('Det Error')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1,1].semilogy(history['lr'])\n",
    "axes[1,1].set_xlabel('Epoch'); axes[1,1].set_ylabel('LR'); axes[1,1].set_title('Learning Rate')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "if best_state:\n",
    "    model.load_state_dict(best_state)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    x_eval = torch.rand(10000, 7, device=device)\n",
    "    det_eval = model.det_g(x_eval)\n",
    "    adjoint_eval = model.get_adjoint_params(x_eval)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"det(g) mean: {det_eval.mean().item():.8f}\")\n",
    "print(f\"det(g) target: {DET_G_TARGET_FLOAT:.8f}\")\n",
    "print(f\"det(g) error: {abs(det_eval.mean().item() - DET_G_TARGET_FLOAT):.2e}\")\n",
    "print(f\"Best torsion: {best_torsion:.6f}\")\n",
    "print(f\"Joyce threshold: {TORSION_THRESHOLD}\")\n",
    "print(f\"Margin: {TORSION_THRESHOLD / best_torsion:.1f}x\" if best_torsion > 0 else \"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(det_eval.cpu().numpy(), bins=50, density=True, alpha=0.7)\n",
    "axes[0].axvline(DET_G_TARGET_FLOAT, color='r', linestyle='--', label='Target 65/32')\n",
    "axes[0].set_xlabel('det(g)'); axes[0].set_title('det(g) Distribution'); axes[0].legend()\n",
    "\n",
    "axes[1].hist(adjoint_eval.cpu().numpy().flatten(), bins=50, density=True, alpha=0.7)\n",
    "axes[1].set_xlabel('Adjoint params'); axes[1].set_title('G2 Adjoint Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'best_state_dict': best_state,\n",
    "    'history': history,\n",
    "    'best_torsion': best_torsion,\n",
    "    'config': {\n",
    "        'num_frequencies': 32,\n",
    "        'hidden_dims': [128, 128, 128],\n",
    "        'perturbation_scale': 0.01,\n",
    "    }\n",
    "}, 'gift_pinn_trained.pt')\n",
    "\n",
    "print(\"Model saved to: gift_pinn_trained.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torsion_ok = best_torsion < TARGET_TORSION\n",
    "det_ok = abs(det_eval.mean().item() - DET_G_TARGET_FLOAT) < TARGET_DET_ERROR\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  GIFT-Native PINN Training Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  [{'X' if torsion_ok else ' '}] Torsion < {TARGET_TORSION}: {best_torsion:.6f}\")\n",
    "print(f\"  [{'X' if det_ok else ' '}] |det(g) - 65/32| < {TARGET_DET_ERROR}: {abs(det_eval.mean().item() - DET_G_TARGET_FLOAT):.2e}\")\n",
    "print()\n",
    "if torsion_ok and det_ok:\n",
    "    print(\"SUCCESS: All criteria met!\")\n",
    "else:\n",
    "    print(\"Training may need more epochs.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}